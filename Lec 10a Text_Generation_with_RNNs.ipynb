{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Text Generation with RNNs.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFVkhvwOKGzk",
        "colab_type": "text"
      },
      "source": [
        "# Text Generation with RNNs\n",
        "\n",
        "This notebook aims: <p>\n",
        "    \n",
        "- to demonstrate how to generate text <p>\n",
        "    \n",
        "- using a character-based RNN <p>\n",
        "\n",
        "- on a large corpus (German philosopher from the 1890s Nietzsche's writings). <p>\n",
        "    \n",
        "Thus for instance, given a sequence of characters from this data ...\n",
        "> \"Shakespear_\"\n",
        "we train a model to predict the next character in the sequence (\"e\"). \n",
        "    \n",
        "Longer sequences of text can be generated by calling the model repeatedly.\n",
        "\n",
        "**Note**: Enable GPU acceleration to execute this notebook faster. In Colab: *Runtime > Change runtime type > Hardware acclerator > GPU*. \n",
        "If running locally make sure TensorFlow version >= 1.11.\n",
        "\n",
        "Ready?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7oCMULaPKGzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "keras.__version__\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bbm7WfhvKGz7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "f0138ad9-8423-4c63-e5c5-3591975a5418"
      },
      "source": [
        "import keras\n",
        "import numpy as np\n",
        "import time\n",
        "\n",
        "# download inbuilt dataset on NietZsche's writings\n",
        "start = time.time()\n",
        "path = keras.utils.get_file(\n",
        "    'nietzsche.txt',\n",
        "    origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
        "text = open(path).read().lower()\n",
        "\n",
        "end = time.time()\n",
        "print(end - start, \" secs\\n\")  # 3.3 secs\n",
        "\n",
        "print('Corpus length in characters:', len(text))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.004668712615966797  secs\n",
            "\n",
            "Corpus length in characters: 600893\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAymIoHgKG0I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "a6d3af8d-367a-4474-db00-c530cdaf0c5c"
      },
      "source": [
        "# View what Nietzsche's writings are like\n",
        "\n",
        "print(text[:400])  # first 600 of 600k characters"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "preface\n",
            "\n",
            "\n",
            "supposing that truth is a woman--what then? is there not ground\n",
            "for suspecting that all philosophers, in so far as they have been\n",
            "dogmatists, have failed to understand women--that the terrible\n",
            "seriousness and clumsy importunity with which they have usually paid\n",
            "their addresses to truth, have been unskilled and unseemly methods for\n",
            "winning a woman? certainly she has never allowed herself \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZaPczS-6KG0S",
        "colab_type": "text"
      },
      "source": [
        "Can one learn about Nietzsche from his writings above: (i) writing style? (ii) propensity to use large words? (iii) clarity vs opaqueness, complexity vs simplicity etc in prose? \n",
        "\n",
        "Q is when we *generate* text, will machine learn and reproduce in generated text reflect these linguistic styles?\n",
        "\n",
        "### Preparing Data for an RNN\n",
        "\n",
        "OK. Back to more mundane things. Recall that RNNs take inputs as *sequences* which will have some length. \n",
        "\n",
        "Next steps, we will: \n",
        "\n",
        "(i) extract partially-overlapping sequences of length `maxlen` (say, 60 chars), <p>\n",
        "(ii) one-hot encode them and <p>\n",
        "(iii) pack them in a 3D Numpy array `x` of shape (`sequences, maxlen, unique_characters`). <p>\n",
        "(iv) Simultaneously, we prepare a array `y` containing the corresponding targets: the one-hot encoded characters that come right after each extracted sequence.<p>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YscmRJ-KG0U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "550df128-aef2-4197-8fe0-0634b8f9dbce"
      },
      "source": [
        "# Length of extracted character sequences\n",
        "maxlen = 60\n",
        "\n",
        "# We sample a new sequence every `step` characters\n",
        "step = 3\n",
        "\n",
        "# This holds our extracted sequences\n",
        "sentences = []\n",
        "\n",
        "# This holds the targets (the follow-up characters)\n",
        "next_chars = []\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "for i in range(0, len(text) - maxlen, step):\n",
        "    sentences.append(text[i: i + maxlen])\n",
        "    next_chars.append(text[i + maxlen])\n",
        "print('Number of sequences:', len(sentences), \"\\n\")\n",
        "\n",
        "# List of unique characters in the corpus\n",
        "chars = sorted(list(set(text)))\n",
        "print('Unique characters:', len(chars), \"\\n\")\n",
        "\n",
        "# Dictionary mapping unique characters to their index in `chars`\n",
        "char_indices = dict((char, chars.index(char)) for char in chars)\n",
        "\n",
        "# Next, one-hot encode the characters into binary arrays.\n",
        "print('Vectorization...')\n",
        "x = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
        "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
        "\n",
        "for i, sentence in enumerate(sentences):\n",
        "    for t, char in enumerate(sentence):\n",
        "        x[i, t, char_indices[char]] = 1\n",
        "    y[i, char_indices[next_chars[i]]] = 1\n",
        "    \n",
        "end = time.time()\n",
        "print(end - start, \" secs\\n\")  # 5.9 secs    "
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of sequences: 200278 \n",
            "\n",
            "Unique characters: 57 \n",
            "\n",
            "Vectorization...\n",
            "3.75154709815979  secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LAGFffsKG0b",
        "colab_type": "text"
      },
      "source": [
        "## Build an LSTM RNN network\n",
        "\n",
        "Our network is a single `LSTM` layer followed by a `Dense` classifier and `softmax` over all possible characters. \n",
        "\n",
        "P.S. Note that RNNs are not the only way to do sequence data generation; 1D convnets also have proven extremely successful at it in recent times."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZU3RyU9KG0e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import layers\n",
        "\n",
        "model = keras.models.Sequential()\n",
        "model.add(layers.LSTM(128, input_shape=(maxlen, len(chars))))\n",
        "model.add(layers.Dense(len(chars), activation='softmax'))\n",
        "\n",
        "optimizer = keras.optimizers.RMSprop(lr=0.01)\n",
        "\n",
        "# using categ_crossEntropy since our Y is categorical\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optimizer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ak3gjmO_KG0m",
        "colab_type": "text"
      },
      "source": [
        "### The Idea of *Temperature*\n",
        "\n",
        "*Temperature* in RNN lingo is a means to *control* the amount of randomness in generative modeling.\n",
        "\n",
        "Given an input sequence, the model learns the probability distribution of finding the next element in that sequence. It generates the next element by sampling from this probability distribution. \n",
        "\n",
        "However, how much leeway we allow in that sampling vastly affects outputs.\n",
        "\n",
        "At one extreme is **greedy sampling** which picks up only the most likely next element *everytime*. This makes text predictable, repeated and boring.\n",
        "\n",
        "At the other extreme is **random sampling** which disregards the learned probability distribution and randomly draws the next element (say, character from a uniform distribution). This makes text generated random and meaningless.\n",
        "\n",
        "Between these 2 extremes, we can set the level of randomness at which the sampler chooses the next element. This is temperature. At low temps, we are closer to the Greedy sampling extreme and athigher temps at the other extreme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sEH4WKE_KG0o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## Training the language model and sampling from it\n",
        "\n",
        "# define a sampl;ing func given temp\n",
        "def sample(preds, temperature=1.0):\n",
        "    preds = np.asarray(preds).astype('float64')\n",
        "    preds = np.log(preds) / temperature\n",
        "    exp_preds = np.exp(preds)\n",
        "    preds = exp_preds / np.sum(exp_preds)\n",
        "    probas = np.random.multinomial(1, preds, 1)\n",
        "    return np.argmax(probas)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDS-QNzEKG0z",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        },
        "outputId": "c1a89b26-d696-4ff0-a563-ec85a6634df2"
      },
      "source": [
        "## Training Loop\n",
        "\n",
        "import random\n",
        "import sys\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# restricting to just 1 epochs due to time paucity\n",
        "for epoch in range(1, 2):\n",
        "    print('epoch', epoch)\n",
        "    \n",
        "    # Fit the model for 1 epoch on the available training data\n",
        "    model.fit(x, y,\n",
        "              batch_size=128,\n",
        "              epochs=1)\n",
        "\n",
        "    # Select a text seed at random\n",
        "    start_index = random.randint(0, len(text) - maxlen - 1)\n",
        "    generated_text = text[start_index: start_index + maxlen]\n",
        "    print('--- Generating with seed: \"' + generated_text + '\"')\n",
        "\n",
        "    for temperature in [0.5, 1.5]:\n",
        "        print('------ temperature:', temperature)\n",
        "        sys.stdout.write(generated_text)\n",
        "\n",
        "        # We generate 400 characters\n",
        "        for i in range(400):\n",
        "            sampled = np.zeros((1, maxlen, len(chars)))\n",
        "            for t, char in enumerate(generated_text):\n",
        "                sampled[0, t, char_indices[char]] = 1.\n",
        "\n",
        "            preds = model.predict(sampled, verbose=0)[0]\n",
        "            next_index = sample(preds, temperature)\n",
        "            next_char = chars[next_index]\n",
        "\n",
        "            generated_text += next_char\n",
        "            generated_text = generated_text[1:]\n",
        "\n",
        "            sys.stdout.write(next_char)\n",
        "            sys.stdout.flush()\n",
        "        print()\n",
        "        \n",
        "end = time.time()\n",
        "print(end - start, \" secs\\n\")  # 5.9 secs           "
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "epoch 1\n",
            "Epoch 1/1\n",
            "200278/200278 [==============================] - 144s 720us/step - loss: 1.9912\n",
            "--- Generating with seed: \"g nuances of free, free-spirited thought. and just\n",
            "as the bu\"\n",
            "------ temperature: 0.5\n",
            "g nuances of free, free-spirited thought. and just\n",
            "as the but to be the heloge of his one is a more must from the comperhander and resent of the present some the among of the danger sould thing is the general to be be and in the some and so in the grane must think of the fact of the seems prises of the seed the germanys of its stincting and the fasted or a conterned, thus its man for the even in the words and not soul have the religion of the believed the \n",
            "------ temperature: 1.5\n",
            "he words and not soul have the religion of the believed the \"it. in it\n",
            "tous, than fe1se y: krisg,mingouen\" andefi7kank and\n",
            ". parical, as perhaps love vilons,--\n",
            "per unpen payoud is to he havere-n\n",
            "'emsrind\n",
            "be un frop tealt demulurs indeienh for\n",
            "opundofy kindiarswet of found asamplet comked\n",
            "of fa3ece of tytemoalinoy ruamcaniwit?\n",
            "hast prrvay. \"therefven unjuthter, the mbpts whico,iones,\" \"\"goroudabse,.n\n",
            "fos\n",
            "con conquedk as no to that,s it, mystfic, im!trminie-\n",
            "173.9379825592041  secs\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nInBJT7kKG07",
        "colab_type": "text"
      },
      "source": [
        "Takes long for each epoch, so restricted to just 2 epochs. However, if we run this long enough, quite interesting pattern emerge at the right temperatures."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7P0RVLMKG09",
        "colab_type": "text"
      },
      "source": [
        "### Example of training at 4 temps in Epoch 41\n",
        "\n",
        "epoch 41\n",
        "Epoch 1/1\n",
        "200278/200278 [==============================] - 125s - loss: 1.3371   \n",
        "- Generating with seed: \"l state. men when coming out of the spell, or resting from s\"\n",
        "\n",
        "------ temperature: 0.2\n",
        "l state. men when coming out of the spell, or resting from\n",
        "self-appear to the superiority of the sense of the sense of the same a sould to the same a subject and problem of the sense\n",
        "of the world of the commonless of the problem of the sense of an action of the same a soul and also the superiority of the the commanded and comparison of the same an exception of the belief the strives and problem of the same only to the sense of the same a sort of the superi\n",
        "\n",
        "------ temperature: 0.5\n",
        " the same only to the sense of the same a sort of the superiority of the problem of comparison and conscience and belief to the fine order of the world all all the things\"\"--they are present to the the problem of constitute oneself, at the world\"\"--all the entirely expended and a possible to spirit and disguise of sense something of the development of an end,\" significance and the sense of the the belief to presumption of the confutent to the higher\n",
        "spirit\n",
        "\n",
        "------ temperature: 1.0\n",
        " belief to presumption of the confutent to the higher\n",
        "spirit whatever all supervalical most to have creesely. i have\n",
        "grow digtised to the virtuous, however, unforenlatomed the develop percet or \"modern eart\"ered more\n",
        "same alway i believe\n",
        "the\n",
        "surks and woman! undees wordsal freor,\n",
        "must do aride ranks \"spardished perdocated!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3WEFWgqjKG0_",
        "colab_type": "text"
      },
      "source": [
        "### Notes from the Above\n",
        "\n",
        "- a low temperature results in extremely repetitive and predictable text, but where local structure is highly realistic: in particular, all words (a word being a local pattern of characters) are real English words. <p>\n",
        "    \n",
        "- With a high temperature, the local structure starts breaking down and most words look like semi-random strings of characters.<p>    \n",
        "    \n",
        "- With higher temperatures, the generated text becomes more interesting, surprising, even creative; it may sometimes invent completely new words that sound somewhat plausible (such as \"eterned\" or \"troveration\"). <p>\n",
        "    \n",
        "- Note that by training a bigger model, longer, on more data, you can achieve generated samples that will look much more coherent and realistic than above. But of course, don't expect to ever generate any meaningful text, other than by random chance: all we are doing is sampling data from a statistical model of which characters come after which characters.    \n",
        "\n",
        "Chalo, back to the slides.\n",
        "    \n",
        "Voleti"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pw7OthtvKG1B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}