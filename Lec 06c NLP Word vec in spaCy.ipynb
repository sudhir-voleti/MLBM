{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Word Vector Models\n",
    "\n",
    "Hi all,\n",
    "\n",
    "Aim here is to demo different NLP functionalities fom spaCy. Below I demo the word vector representations capability.\n",
    "\n",
    "Stand-alone words (e.g., \"bank\") by themselves often can't convey much info because *context* is missing (e.g., \"commercial bank\" or \"river bank\").\n",
    "\n",
    "For large numbers of common words, some idea about context (and relative probabilities of occurring in differenrt contexts) can be had if models can be trained on large enough corpora (e.g., Wikipedia corpus, Goog News, Goog books etc.).\n",
    "\n",
    "Imagine building a colossally high-D **vocabulary** space wherein words similar in meaning and context are closer together than to other words. \n",
    "\n",
    "**Word embeddings** are real-number co-ordinate vectors representing a vocab's words in such a space (see example below).\n",
    "\n",
    "To be useful, a set of word vectors for a vocabulary should capture (a) the meaning of words, (b) the relationship between words, and (c) the context of different words (d) as they are used naturally.\n",
    "\n",
    "They allow us to implicitly include external information from the world into our language understanding models. They thus have broad NLP applications in areas like sentiment-an, text classification etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup chunk\n",
    "# !python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# nlp = en_core_web_md.load()\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Word Vec in spaCy\n",
    "\n",
    "Using pre-trained models in Spacy is incredible convenient, given that they come built in. \n",
    "\n",
    "Spacy has a number of different models of different sizes available for use, with models in 7 different languages (include English, Polish, German, Spanish, Portuguese, French, Italian, and Dutch), and of different sizes.\n",
    "\n",
    "For English, we'll install the large *en_core_web_md* library, which includes 20k unique vectors with 384 dimensions.\n",
    "\n",
    "### Invoking Word Vecs of Particular Tokens\n",
    "\n",
    "Below I demo how to invoke basic word-vec functionality in spaCy. Behold.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma</th>\n",
       "      <th>postag</th>\n",
       "      <th>depcy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>conj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>should</td>\n",
       "      <td>should</td>\n",
       "      <td>VERB</td>\n",
       "      <td>aux</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>be</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>closer</td>\n",
       "      <td>close</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>advmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>together</td>\n",
       "      <td>together</td>\n",
       "      <td>ADV</td>\n",
       "      <td>advmod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>in</td>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>prep</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>vocab</td>\n",
       "      <td>vocab</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>amod</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>space</td>\n",
       "      <td>space</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pobj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>as</td>\n",
       "      <td>as</td>\n",
       "      <td>ADP</td>\n",
       "      <td>mark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>should</td>\n",
       "      <td>should</td>\n",
       "      <td>VERB</td>\n",
       "      <td>advcl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Mango</td>\n",
       "      <td>mango</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>nsubj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>CCONJ</td>\n",
       "      <td>cc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Banana</td>\n",
       "      <td>banana</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>conj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>.</td>\n",
       "      <td>.</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>punct</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text     lemma postag   depcy\n",
       "0        Cat       cat  PROPN   nsubj\n",
       "1        and       and  CCONJ      cc\n",
       "2        Dog       dog  PROPN    conj\n",
       "3     should    should   VERB     aux\n",
       "4         be        be   VERB    ROOT\n",
       "5     closer     close    ADJ  advmod\n",
       "6   together  together    ADV  advmod\n",
       "7         in        in    ADP    prep\n",
       "8      vocab     vocab   NOUN    amod\n",
       "9      space     space   NOUN    pobj\n",
       "10        as        as    ADP    mark\n",
       "11    should    should   VERB   advcl\n",
       "12     Mango     mango  PROPN   nsubj\n",
       "13       and       and  CCONJ      cc\n",
       "14    Banana    banana  PROPN    conj\n",
       "15         .         .  PUNCT   punct"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# process a sentence using the model\n",
    "sent = \"Cat and Dog should be closer together in vocab space as should Mango and Banana.\"\n",
    "\n",
    "# routine to display sentence as DF postags\n",
    "def token_attrib(sent0):\n",
    "\tdoc = nlp(sent0)\n",
    "\n",
    "\ttext=[]; lemma=[]; postag=[]; depcy=[]\n",
    "\n",
    "\tfor token in doc:\n",
    "\t\ttext.append(token.text)\n",
    "\t\tlemma.append(token.lemma_)\n",
    "\t\tpostag.append(token.pos_)\n",
    "\t\tdepcy.append(token.dep_)\n",
    "\n",
    "\ttest_df = pd.DataFrame({'text':text, 'lemma':lemma, 'postag':postag, 'depcy':depcy})\n",
    "\treturn(test_df)\n",
    "\n",
    "# display the outp DF\n",
    "token_attrib(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Third token in sentence is:  Dog \n",
      "\n",
      "Word-vector shape for the token:  Dog  is:\n",
      " (384,) \n",
      "\n",
      "First 10 word-vec co-ords for the token * Dog * are:\n",
      " [-1.9289579   4.1713147   3.2283213   2.3073325  -0.2678644   0.39294454\n",
      "  0.4357926  -3.7268226  -1.0738019   0.3235892 ]\n",
      "==========\n",
      "\n",
      "First 10 word-vec co-ords for the token * Cat * are:\n",
      " [ 1.143025    0.07028291  2.5424826   1.9754298  -1.2145972   3.1527033\n",
      "  0.9029567  -2.3866327   1.2338722  -0.5432149 ]\n"
     ]
    }
   ],
   "source": [
    "# It's that simple - all of the vectors and words are assigned after this point. Get the vector for 'Dog':\n",
    "sent_ann = nlp(sent)  # annotate sent\n",
    "print(\"Third token in sentence is: \", sent_ann[2], \"\\n\")\n",
    "\n",
    "# check word vec length\n",
    "print(\"Word-vector shape for the token: \", sent_ann[2], \" is:\\n\", sent_ann[2].vector.shape, \"\\n\")  # 384x1\n",
    "\n",
    "# view a few tokens \n",
    "print(\"First 10 word-vec co-ords for the token *\", sent_ann[2], \"* are:\\n\", sent_ann[2].vector[:10])\n",
    "print(\"==========\\n\")\n",
    "print(\"First 10 word-vec co-ords for the token *\", sent_ann[0], \"* are:\\n\", sent_ann[0].vector[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we humans can't tell that those numbers out there represent 'dogs' and 'cats' respectively, no such problem for machines.\n",
    "\n",
    "Further, recognize what we have just achieved - reducing words to location co-ordinates in vocabulary space! \n",
    "\n",
    "Analogous to how reducing text under bag-of-words (BOW) model to Document Token matrices (DTMs) enabled application of all variety of matrix ops to text collections. \n",
    "\n",
    "IOW, with word-embeddings giving word locations in vocab space, the entire toolkit for spatial analytics can be brought to bear to analyze things like: <p>\n",
    "- similarities between words (how close are they in this space) <p>\n",
    "- differences between words (how far apart) <p>\n",
    "- context around words (which words occur most *around* focal words) <p>\n",
    "- vector sums of word-aggregates (in a sentence, for example) <p>\n",
    "- etc! <p>\n",
    "\n",
    "### Computing Word-Vec similarity\n",
    "\n",
    "A quick dash to the slides for the concept of 'cosine similarity' and let's return once there.\n",
    "\n",
    "Let's compute cosine similarity between words of interest viz. cats, dogs, daisies, lilies etc. Behold.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word1</th>\n",
       "      <th>word2</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dog</td>\n",
       "      <td>dog</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>dog</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.503085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>dog</td>\n",
       "      <td>banana</td>\n",
       "      <td>0.292889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>dog</td>\n",
       "      <td>rose</td>\n",
       "      <td>0.152840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dog</td>\n",
       "      <td>lily</td>\n",
       "      <td>0.172790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dog</td>\n",
       "      <td>cauliflower</td>\n",
       "      <td>0.126985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>cat</td>\n",
       "      <td>dog</td>\n",
       "      <td>0.503085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cat</td>\n",
       "      <td>cat</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cat</td>\n",
       "      <td>banana</td>\n",
       "      <td>0.471543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>cat</td>\n",
       "      <td>rose</td>\n",
       "      <td>0.072137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>cat</td>\n",
       "      <td>lily</td>\n",
       "      <td>0.070799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>cat</td>\n",
       "      <td>cauliflower</td>\n",
       "      <td>0.114891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>banana</td>\n",
       "      <td>dog</td>\n",
       "      <td>0.292889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>banana</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.471543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>banana</td>\n",
       "      <td>banana</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>banana</td>\n",
       "      <td>rose</td>\n",
       "      <td>0.086525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>banana</td>\n",
       "      <td>lily</td>\n",
       "      <td>0.244331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>banana</td>\n",
       "      <td>cauliflower</td>\n",
       "      <td>0.090779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>rose</td>\n",
       "      <td>dog</td>\n",
       "      <td>0.152840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>rose</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.072137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>rose</td>\n",
       "      <td>banana</td>\n",
       "      <td>0.086525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>rose</td>\n",
       "      <td>rose</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>rose</td>\n",
       "      <td>lily</td>\n",
       "      <td>0.054049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>rose</td>\n",
       "      <td>cauliflower</td>\n",
       "      <td>-0.005698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>lily</td>\n",
       "      <td>dog</td>\n",
       "      <td>0.172790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>lily</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.070799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>lily</td>\n",
       "      <td>banana</td>\n",
       "      <td>0.244331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>lily</td>\n",
       "      <td>rose</td>\n",
       "      <td>0.054049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>lily</td>\n",
       "      <td>lily</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>lily</td>\n",
       "      <td>cauliflower</td>\n",
       "      <td>0.469654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>cauliflower</td>\n",
       "      <td>dog</td>\n",
       "      <td>0.126985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>cauliflower</td>\n",
       "      <td>cat</td>\n",
       "      <td>0.114891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>cauliflower</td>\n",
       "      <td>banana</td>\n",
       "      <td>0.090779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>cauliflower</td>\n",
       "      <td>rose</td>\n",
       "      <td>-0.005698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>cauliflower</td>\n",
       "      <td>lily</td>\n",
       "      <td>0.469654</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>cauliflower</td>\n",
       "      <td>cauliflower</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          word1        word2  similarity\n",
       "0           dog          dog    1.000000\n",
       "1           dog          cat    0.503085\n",
       "2           dog       banana    0.292889\n",
       "3           dog         rose    0.152840\n",
       "4           dog         lily    0.172790\n",
       "5           dog  cauliflower    0.126985\n",
       "6           cat          dog    0.503085\n",
       "7           cat          cat    1.000000\n",
       "8           cat       banana    0.471543\n",
       "9           cat         rose    0.072137\n",
       "10          cat         lily    0.070799\n",
       "11          cat  cauliflower    0.114891\n",
       "12       banana          dog    0.292889\n",
       "13       banana          cat    0.471543\n",
       "14       banana       banana    1.000000\n",
       "15       banana         rose    0.086525\n",
       "16       banana         lily    0.244331\n",
       "17       banana  cauliflower    0.090779\n",
       "18         rose          dog    0.152840\n",
       "19         rose          cat    0.072137\n",
       "20         rose       banana    0.086525\n",
       "21         rose         rose    1.000000\n",
       "22         rose         lily    0.054049\n",
       "23         rose  cauliflower   -0.005698\n",
       "24         lily          dog    0.172790\n",
       "25         lily          cat    0.070799\n",
       "26         lily       banana    0.244331\n",
       "27         lily         rose    0.054049\n",
       "28         lily         lily    1.000000\n",
       "29         lily  cauliflower    0.469654\n",
       "30  cauliflower          dog    0.126985\n",
       "31  cauliflower          cat    0.114891\n",
       "32  cauliflower       banana    0.090779\n",
       "33  cauliflower         rose   -0.005698\n",
       "34  cauliflower         lily    0.469654\n",
       "35  cauliflower  cauliflower    1.000000"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# higher the cosine-similairity score, more the words contextually relate to each other\n",
    "tokens = nlp(\"dog cat banana rose lily cauliflower\")  # annotate doc\n",
    "\n",
    "text1=[]; text2=[]; simil_score=[]\n",
    "# for loop and print\n",
    "for token1 in tokens:\n",
    "    for token2 in tokens:\n",
    "        text1.append(token1.text)\n",
    "        text2.append(token2.text)\n",
    "        simil_score.append(token1.similarity(token2))\n",
    "\n",
    "# build DF for neat output\n",
    "simil_df = pd.DataFrame({'word1':text1, 'word2':text2, 'similarity': simil_score} )        \n",
    "simil_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results above are along expected lines I hope. \n",
    "\n",
    "Again, imp to note that above vocab space was built based on common vocab words found in massive corpora online by the likes of <p>\n",
    "- [Google's Word2vec model](https://code.google.com/archive/p/word2vec/), <p>\n",
    "- [Stanford's Global Vectors for Word Representation GloVe model](https://nlp.stanford.edu/projects/glove/) <p>\n",
    "- and even [Facebook's fasttext algo](https://github.com/facebookresearch/fastText). <p>\n",
    "So they aren't infalliable but reflect the contexts which they trained on.\n",
    "\n",
    "Sure, one can and should train word embeddings specific to one's business domain for better prediction and explanation. spaCy (and *gensim*) allow for such custom-building of vocabs.\n",
    "\n",
    "### Vector Sums in Metric Vocab Spaces\n",
    "\n",
    "Vectors can be added and subtracted in metric spaces (which vocab spaces are in 300+ dimns). They can also be multiplied and divided (dot- & cross- products and inverses, for example).\n",
    "\n",
    "Below I demo what happens when we do Vector summation over natural word groupings (e.g., words in a sentence) to obtain a document level vocab space vector. \n",
    "\n",
    "spaCy makes this incredibly convenient via the extension '.vector' to the annotated document. Result is the mean or average of the vector sum of all component tokens. \n",
    "\n",
    "Behold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.15729392  0.31356293  0.5392628   1.5597563   1.2362208   0.4116027\n",
      "  0.43689406 -0.04755433] \n",
      "\n",
      "[ 0.30408886  0.1734363   0.9671667   4.356767    1.422149   -0.48655793\n",
      " -0.18334758 -0.6775466 ] \n",
      "\n",
      "[ 0.4242241   1.6088556   1.5877622   1.8958215  -0.07814423  1.3804523\n",
      " -1.5191032  -2.0572953 ] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# simulate 3 sentences\n",
    "sent1 = \"Mozart's ninth symphony is considered his best.\"\n",
    "sent2 = \"The Unforgiven is a cult western.\"\n",
    "sent3 = \"Machine Learning basics for Managers.\"\n",
    "sent4 = \"Elon Musk wants to colonize Mars.\"\n",
    "\n",
    "# use nlp() to annotate them into nlp objects\n",
    "doc1 = nlp(sent1)\n",
    "doc2 = nlp(sent2)\n",
    "doc3 = nlp(sent3)\n",
    "doc4 = nlp(sent4)\n",
    "\n",
    "# view a few nums from the mean vector for the entire sentence \n",
    "# (useful for sentence classification etc.)\n",
    "print(doc1.vector[:8],\"\\n\")  \n",
    "print(doc2.vector[:8],\"\\n\")  \n",
    "print(doc3.vector[:8],\"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent1</th>\n",
       "      <th>sent2</th>\n",
       "      <th>simil_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mozart's ninth symphony is considered his best.</td>\n",
       "      <td>The Unforgiven is a cult western.</td>\n",
       "      <td>0.626263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Mozart's ninth symphony is considered his best.</td>\n",
       "      <td>Machine Learning basics for Managers.</td>\n",
       "      <td>0.281639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Mozart's ninth symphony is considered his best.</td>\n",
       "      <td>Elon Musk wants to colonize Mars.</td>\n",
       "      <td>0.415729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The Unforgiven is a cult western.</td>\n",
       "      <td>The Unforgiven is a cult western.</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Unforgiven is a cult western.</td>\n",
       "      <td>Machine Learning basics for Managers.</td>\n",
       "      <td>0.398735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The Unforgiven is a cult western.</td>\n",
       "      <td>Elon Musk wants to colonize Mars.</td>\n",
       "      <td>0.456670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Machine Learning basics for Managers.</td>\n",
       "      <td>The Unforgiven is a cult western.</td>\n",
       "      <td>0.398735</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Machine Learning basics for Managers.</td>\n",
       "      <td>Machine Learning basics for Managers.</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Machine Learning basics for Managers.</td>\n",
       "      <td>Elon Musk wants to colonize Mars.</td>\n",
       "      <td>0.596288</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             sent1  \\\n",
       "0  Mozart's ninth symphony is considered his best.   \n",
       "1  Mozart's ninth symphony is considered his best.   \n",
       "2  Mozart's ninth symphony is considered his best.   \n",
       "3                The Unforgiven is a cult western.   \n",
       "4                The Unforgiven is a cult western.   \n",
       "5                The Unforgiven is a cult western.   \n",
       "6            Machine Learning basics for Managers.   \n",
       "7            Machine Learning basics for Managers.   \n",
       "8            Machine Learning basics for Managers.   \n",
       "\n",
       "                                   sent2  simil_score  \n",
       "0      The Unforgiven is a cult western.     0.626263  \n",
       "1  Machine Learning basics for Managers.     0.281639  \n",
       "2      Elon Musk wants to colonize Mars.     0.415729  \n",
       "3      The Unforgiven is a cult western.     1.000000  \n",
       "4  Machine Learning basics for Managers.     0.398735  \n",
       "5      Elon Musk wants to colonize Mars.     0.456670  \n",
       "6      The Unforgiven is a cult western.     0.398735  \n",
       "7  Machine Learning basics for Managers.     1.000000  \n",
       "8      Elon Musk wants to colonize Mars.     0.596288  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now examine which sents are closer to each other in context / meaning terms as per word-vec \n",
    "sent_1=[]; sent_2=[]; simil_score=[]\n",
    "\n",
    "for i in [1, 2, 3]:\n",
    "    for j in [2, 3, 4]:\n",
    "        sent_1.append(eval(str(\"sent\" + str(i))))\n",
    "        sent_2.append(eval(str(\"sent\" + str(j))))\n",
    "        simil_score.append(eval(str(\"doc\" + str(i))).similarity(eval(str(\"doc\" + str(j)))))\n",
    "        \n",
    "# store n display as DF\n",
    "sent_simil_df = pd.DataFrame({'sent1':sent_1, 'sent2':sent_2, 'simil_score':simil_score})        \n",
    "sent_simil_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, what do you think? Do the similarities make common sense? \n",
    "\n",
    "We can compare similarity between text aggregates such as paragraphs and documents just as well. \n",
    "\n",
    "Recall from MKTR we did cluster-an on the DTM to find docs that were 'similar' in their word features. \n",
    "\n",
    "This is different in that we're using externally trained models to implicitly provide context during the similarity score computation.\n",
    "\n",
    "Chalo, back to the slides.\n",
    "\n",
    "Voleti"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
